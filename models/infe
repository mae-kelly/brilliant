import asyncio
import uvicorn
import time
import numpy as np
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import List, Dict, Optional, Any
import os
import sys
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from model_inference import model_inference, PredictionResult

app = FastAPI(
    title="Renaissance ML Inference Server",
    description="High-performance ML inference server for DeFi momentum trading",
    version="2.0.0"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class SinglePredictionRequest(BaseModel):
    features: List[float] = Field(..., description="Feature vector (45 dimensions)")
    token_address: Optional[str] = Field("", description="Token address for caching")

class BatchPredictionRequest(BaseModel):
    feature_batches: List[List[float]] = Field(..., description="Batch of feature vectors")
    token_addresses: Optional[List[str]] = Field(None, description="Corresponding token addresses")

class PredictionResponse(BaseModel):
    breakout_probability: float
    confidence: float
    entropy: float
    regime_state: int
    regime_confidence: float
    feature_importance: Dict[str, float]
    execution_time_ms: float
    model_version: str

class BatchPredictionResponse(BaseModel):
    predictions: List[PredictionResponse]
    batch_size: int
    total_execution_time_ms: float

class PerformanceUpdateRequest(BaseModel):
    prediction_id: str
    breakout_probability: float
    actual_outcome: bool
    confidence: float

class ModelStats(BaseModel):
    model_type: str
    model_version: str
    total_predictions: int
    cache_hit_rate: float
    avg_inference_time_ms: float
    predictions_per_second: float
    model_accuracy: float
    uptime_minutes: float

class HealthResponse(BaseModel):
    status: str
    model_loaded: bool
    server_uptime: float
    last_prediction_time: Optional[float]

inference_server_stats = {
    'requests_processed': 0,
    'batch_requests_processed': 0,
    'total_predictions': 0,
    'start_time': time.time(),
    'last_prediction_time': None,
    'error_count': 0,
    'performance_updates': 0
}

@app.on_event("startup")
async def startup_event():
    try:
        success = await model_inference.initialize()
        if not success:
            raise Exception("Model initialization failed")
        
        asyncio.create_task(periodic_cache_cleanup())
        
        print("âœ… Renaissance Inference Server started successfully")
        print(f"ðŸ§  Model type: {model_inference.model_type}")
        print(f"ðŸ“Š Features: {len(model_inference.feature_names)}")
        print(f"ðŸš€ Server ready for high-frequency inference")
        
    except Exception as e:
        print(f"âŒ Server startup failed: {e}")
        raise

@app.post("/predict", response_model=PredictionResponse)
async def predict_single(request: SinglePredictionRequest):
    start_time = time.time()
    
    try:
        if len(request.features) != len(model_inference.feature_names):
            raise HTTPException(
                status_code=400,
                detail=f"Expected {len(model_inference.feature_names)} features, got {len(request.features)}"
            )
        
        features = np.array(request.features, dtype=np.float32)
        
        result = await model_inference.predict_breakout(features, request.token_address)
        
        inference_server_stats['requests_processed'] += 1
        inference_server_stats['total_predictions'] += 1
        inference_server_stats['last_prediction_time'] = time.time()
        
        return PredictionResponse(
            breakout_probability=result.breakout_probability,
            confidence=result.confidence,
            entropy=result.entropy,
            regime_state=result.regime_state,
            regime_confidence=result.regime_confidence,
            feature_importance=result.feature_importance,
            execution_time_ms=result.execution_time_ms,
            model_version=result.model_version
        )
        
    except HTTPException:
        raise
    except Exception as e:
        inference_server_stats['error_count'] += 1
        raise HTTPException(status_code=500, detail=f"Prediction failed: {str(e)}")

@app.post("/predict/batch", response_model=BatchPredictionResponse)
async def predict_batch(request: BatchPredictionRequest):
    batch_start_time = time.time()
    
    try:
        if not request.feature_batches:
            raise HTTPException(status_code=400, detail="Empty batch request")
        
        expected_features = len(model_inference.feature_names)
        for i, features in enumerate(request.feature_batches):
            if len(features) != expected_features:
                raise HTTPException(
                    status_code=400,
                    detail=f"Batch item {i}: Expected {expected_features} features, got {len(features)}"
                )
        
        feature_arrays = [np.array(features, dtype=np.float32) for features in request.feature_batches]
        token_addresses = request.token_addresses or [""] * len(feature_arrays)
        
        if len(token_addresses) != len(feature_arrays):
            token_addresses = [""] * len(feature_arrays)
        
        results = await model_inference.batch_predict(feature_arrays, token_addresses)
        
        batch_execution_time = (time.time() - batch_start_time) * 1000
        
        inference_server_stats['batch_requests_processed'] += 1
        inference_server_stats['total_predictions'] += len(results)
        inference_server_stats['last_prediction_time'] = time.time()
        
        predictions = [
            PredictionResponse(
                breakout_probability=result.breakout_probability,
                confidence=result.confidence,
                entropy=result.entropy,
                regime_state=result.regime_state,
                regime_confidence=result.regime_confidence,
                feature_importance=result.feature_importance,
                execution_time_ms=result.execution_time_ms,
                model_version=result.model_version
            )
            for result in results
        ]
        
        return BatchPredictionResponse(
            predictions=predictions,
            batch_size=len(predictions),
            total_execution_time_ms=batch_execution_time
        )
        
    except HTTPException:
        raise
    except Exception as e:
        inference_server_stats['error_count'] += 1
        raise HTTPException(status_code=500, detail=f"Batch prediction failed: {str(e)}")

@app.post("/performance/update")
async def update_performance(request: PerformanceUpdateRequest, background_tasks: BackgroundTasks):
    try:
        result = PredictionResult(
            breakout_probability=request.breakout_probability,
            confidence=request.confidence,
            entropy=0.693,
            regime_state=0,
            regime_confidence=0.5,
            feature_importance={},
            execution_time_ms=0.0,
            model_version=""
        )
        
        background_tasks.add_task(
            model_inference.update_model_performance,
            result,
            request.actual_outcome
        )
        
        inference_server_stats['performance_updates'] += 1
        
        return {"status": "success", "message": "Performance update queued"}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Performance update failed: {str(e)}")

@app.get("/health", response_model=HealthResponse)
async def health_check():
    uptime = time.time() - inference_server_stats['start_time']
    
    return HealthResponse(
        status="healthy",
        model_loaded=model_inference.tflite_interpreter is not None or model_inference.sklearn_model is not None,
        server_uptime=uptime,
        last_prediction_time=inference_server_stats.get('last_prediction_time')
    )

@app.get("/stats", response_model=ModelStats)
async def get_model_stats():
    try:
        model_stats = model_inference.get_model_statistics()
        uptime_minutes = (time.time() - inference_server_stats['start_time']) / 60
        
        return ModelStats(
            model_type=model_stats.get('model_type', 'unknown'),
            model_version=model_stats.get('model_version', 'unknown'),
            total_predictions=model_stats.get('total_predictions', 0),
            cache_hit_rate=model_stats.get('cache_hit_rate', 0.0),
            avg_inference_time_ms=model_stats.get('avg_inference_time_ms', 0.0),
            predictions_per_second=model_stats.get('predictions_per_second', 0.0),
            model_accuracy=model_stats.get('model_accuracy', 0.0),
            uptime_minutes=uptime_minutes
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Stats retrieval failed: {str(e)}")

@app.get("/stats/detailed")
async def get_detailed_stats():
    try:
        model_stats = model_inference.get_model_statistics()
        cache_stats = model_inference.get_cache_statistics()
        
        uptime = time.time() - inference_server_stats['start_time']
        
        return {
            "server_stats": {
                "uptime_seconds": uptime,
                "requests_processed": inference_server_stats['requests_processed'],
                "batch_requests_processed": inference_server_stats['batch_requests_processed'],
                "total_predictions": inference_server_stats['total_predictions'],
                "error_count": inference_server_stats['error_count'],
                "performance_updates": inference_server_stats['performance_updates'],
                "last_prediction_time": inference_server_stats['last_prediction_time']
            },
            "model_stats": model_stats,
            "cache_stats": cache_stats,
            "performance_metrics": {
                "requests_per_second": inference_server_stats['requests_processed'] / max(uptime, 1),
                "predictions_per_second": inference_server_stats['total_predictions'] / max(uptime, 1),
                "error_rate": inference_server_stats['error_count'] / max(inference_server_stats['requests_processed'], 1)
            }
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Detailed stats failed: {str(e)}")

@app.post("/benchmark")
async def run_benchmark(num_samples: int = 1000):
    try:
        if num_samples > 10000:
            raise HTTPException(status_code=400, detail="Maximum 10,000 samples allowed for benchmark")
        
        benchmark_results = await model_inference.benchmark_inference_speed(num_samples)
        
        return {
            "benchmark_results": benchmark_results,
            "status": "completed",
            "timestamp": time.time()
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Benchmark failed: {str(e)}")

@app.get("/features")
async def get_feature_info():
    return {
        "feature_names": model_inference.feature_names,
        "feature_count": len(model_inference.feature_names),
        "model_type": model_inference.model_type,
        "scaler_available": model_inference.scaler is not None
    }

@app.post("/cache/clear")
async def clear_cache():
    try:
        await model_inference.cleanup_cache()
        
        return {
            "status": "success",
            "message": "Cache cleared successfully",
            "timestamp": time.time()
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Cache clear failed: {str(e)}")

@app.get("/status")
async def get_server_status():
    uptime = time.time() - inference_server_stats['start_time']
    
    return {
        "status": "running",
        "version": "2.0.0",
        "uptime_seconds": uptime,
        "model_loaded": model_inference.tflite_interpreter is not None or model_inference.sklearn_model is not None,
        "model_type": model_inference.model_type,
        "feature_count": len(model_inference.feature_names),
        "total_requests": inference_server_stats['requests_processed'],
        "total_predictions": inference_server_stats['total_predictions'],
        "server_time": time.time()
    }

async def periodic_cache_cleanup():
    while True:
        try:
            await asyncio.sleep(300)
            await model_inference.cleanup_cache()
        except Exception as e:
            print(f"Cache cleanup error: {e}")
            await asyncio.sleep(600)

@app.exception_handler(Exception)
async def global_exception_handler(request, exc):
    inference_server_stats['error_count'] += 1
    
    return {
        "error": "Internal server error",
        "detail": str(exc),
        "timestamp": time.time()
    }

if __name__ == "__main__":
    print("ðŸš€ Starting Renaissance ML Inference Server...")
    print("=" * 60)
    print("ðŸŽ¯ High-performance inference for momentum trading")
    print("âš¡ Features: Real-time predictions, batch processing, performance tracking")
    print("ðŸ§  Models: TensorFlow Lite, scikit-learn, regime detection")
    print("ðŸ“Š Endpoints: /predict, /predict/batch, /stats, /benchmark")
    print("=" * 60)
    
    uvicorn.run(
        app, 
        host="0.0.0.0", 
        port=int(os.getenv("INFERENCE_PORT", 8000)),
        log_level="info",
        access_log=False
    )